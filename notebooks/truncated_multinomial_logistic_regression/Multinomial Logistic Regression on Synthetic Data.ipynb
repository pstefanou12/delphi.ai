{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('../..')\n",
    "from cox.utils import Parameters\n",
    "from cox.store import Store\n",
    "from cox.readers import CollectionReader\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import itertools\n",
    "import numpy as np\n",
    "import torch as ch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Gumbel, Uniform\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import datetime\n",
    "from delphi.oracle import oracle\n",
    "\n",
    "# set default tensor type \n",
    "ch.set_default_tensor_type(ch.cuda.FloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Experiment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# procedure hyperparameters\n",
    "args = Parameters({ \n",
    "    'epochs': 25,\n",
    "    'num_workers': 0, \n",
    "    'batch_size': 100,\n",
    "    'bias': True,\n",
    "    'num_samples': 100000,\n",
    "    'clamp': True, \n",
    "    'radius': 5.0, \n",
    "    'var_lr': 1e-2,\n",
    "    'lr': 1e-1,\n",
    "    'shuffle': False, \n",
    "    'samples': 10000,  # number of samples to generate for ground truth\n",
    "    'in_features': 10, # number of in-features to multi-log-reg\n",
    "    'k': 10, # number of classes\n",
    "    'lower': -1, # lower bound for generating ground truth weights\n",
    "    'upper': 1,  # upper bound for generating ground truth weights\n",
    "    'trials': 10,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CE Latent Variable Model Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "gumbel = Gumbel(0, 1)\n",
    "\n",
    "class GumbelCE(ch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, pred, targ):\n",
    "        ctx.save_for_backward(pred, targ)\n",
    "        loss = ch.nn.CrossEntropyLoss()\n",
    "        return loss(pred, targ)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        pred, targ = ctx.saved_tensors\n",
    "        # make num_samples copies of pred logits\n",
    "        stacked = pred[None, ...].repeat(args.num_samples, 1, 1)        \n",
    "        # add gumbel noise to logits\n",
    "        rand_noise = gumbel.sample(stacked.size())\n",
    "        noised = stacked + rand_noise \n",
    "        noised_labs = noised.argmax(-1)\n",
    "        # remove the logits from the trials, where the kth logit is not the largest value\n",
    "        good_mask = noised_labs.eq(targ)[..., None]\n",
    "        inner_exp = 1 - ch.exp(-rand_noise)\n",
    "        avg = (inner_exp * good_mask).sum(0) / (good_mask.sum(0) + 1e-5) / pred.size(0)\n",
    "        return -avg , None\n",
    "    \n",
    "class TruncatedGumbelCE(ch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, pred, targ, phi):\n",
    "        ctx.save_for_backward(pred, targ)\n",
    "        ctx.phi = phi\n",
    "        ce_loss = ch.nn.CrossEntropyLoss()\n",
    "        return ce_loss(pred, targ)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        pred, targ = ctx.saved_tensors\n",
    "        # make num_samples copies of pred logits\n",
    "        stacked = pred[None, ...].repeat(args.num_samples, 1, 1)   \n",
    "        # add gumbel noise to logits\n",
    "        rand_noise = gumbel.sample(stacked.size())\n",
    "        noised = stacked + rand_noise \n",
    "        # truncate - if one of the noisy logits does not fall within the truncation set, remove it\n",
    "        filtered = ch.all(ctx.phi(noised).bool(), dim=2).float().unsqueeze(2)\n",
    "        noised_labs = noised.argmax(-1)\n",
    "        # mask takes care of invalid logits and truncation set\n",
    "        mask = noised_labs.eq(targ)[..., None] * filtered\n",
    "        inner_exp = 1 - ch.exp(-rand_noise)\n",
    "                \n",
    "        avg = ((inner_exp * mask).sum(0) / (mask.sum(0) + 1e-5) - (inner_exp * filtered).sum(0) / (filtered.sum(0) + 1e-5)) \n",
    "        return -avg / pred.size(0), None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Truncated Multinomial Logistic Regression Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Membership oracles for Multinomial Logistic Regression Logits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN_Lower(oracle): \n",
    "    \"\"\"\n",
    "    Lower bound truncation on the DNN logits.\n",
    "    \"\"\"\n",
    "    def __init__(self, lower): \n",
    "        self.lower = lower\n",
    "        \n",
    "    def __call__(self, x): \n",
    "        return (x > self.lower).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(oracle): \n",
    "    def __call__(self, x): \n",
    "        return ch.ones(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Truncate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = DNN_Lower(ch.full(ch.Size([args.K,]), -7))\n",
    "# phi = Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging in: /home/pstefanou/MultinomialLogisticRegression/2c04de1a-c9d1-4c79-8f6f-a3bac6c9dada\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<cox.store.Table at 0x7faf37373978>"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TRUNC CE LOSS TABLE FOR METRICS\n",
    "LATENT_CE_TABLE_NAME = 'trunc_test_10'\n",
    "\n",
    "STORE_PATH = '/home/pstefanou/MultinomialLogisticRegression'\n",
    "store = Store(STORE_PATH)\n",
    "\n",
    "store.add_table(LATENT_CE_TABLE_NAME, { \n",
    "    'trunc_train_acc': float, \n",
    "    'trunc_val_acc': float, \n",
    "    'trunc_train_loss': float, \n",
    "    'trunc_val_loss': float,\n",
    "    'naive_train_acc': float, \n",
    "    'naive_val_acc': float, \n",
    "    'naive_train_loss': float, \n",
    "    'naive_val_loss': float,\n",
    "    'trunc_test_acc': float, \n",
    "    'naive_test_acc': float,\n",
    "    'epoch': int,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = Uniform(args.lower, args.upper) # distribution to generate ground-truth parameters\n",
    "U_ = Uniform(-5, 5) # distribution to generate samples\n",
    "\n",
    "# perform trials number of experiments\n",
    "for i in range(args.trials):\n",
    "    \n",
    "    # continue to generate synthetic data until survival probability of more than 40%\n",
    "    alpha = None\n",
    "    while alpha is None or alpha < .4:\n",
    "        # generate ground-truth from uniform distribution\n",
    "        ground_truth = nn.Linear(in_features=args.IN_FEATURES, out_features=args.K, bias=args.bias)\n",
    "        ground_truth.weight = nn.Parameter(U.sample(ch.Size([args.K, args.IN_FEATURES])))\n",
    "        if ground_truth.bias is not None: \n",
    "            ground_truth.bias = nn.Parameter(U.sample(ch.Size([args.K,])))\n",
    "        # independent variable \n",
    "        X = U_.sample(ch.Size([args.samples, args.IN_FEATURES]))\n",
    "        # determine base model logits \n",
    "        z = ground_truth(X)\n",
    "        # apply softmax to unnormalized likelihoods\n",
    "        y = ch.argmax(ch.nn.Softmax(dim=1)(z), dim=1)\n",
    "\n",
    "        # TRUNCATE\n",
    "        trunc = phi(z)\n",
    "        indices = ch.all(trunc.bool(), dim=1).float().nonzero(as_tuple=False).flatten()\n",
    "        y_trunc = y[indices]\n",
    "        x_trunc = X[indices]\n",
    "        alpha = x_trunc.size(0) / X.size(0)\n",
    "\n",
    "        # all synthetic data \n",
    "        ds = TensorDataset(x_trunc, y_trunc)\n",
    "        # split ds into training and validation data sets - 80% training, 20% validation\n",
    "        train_length = int(len(ds)*.8)\n",
    "        val_length = len(ds) - train_length\n",
    "        train_ds, val_ds = ch.utils.data.random_split(ds, [train_length, val_length])\n",
    "        # train and validation loaders\n",
    "        train_loader = DataLoader(train_ds, num_workers=args.num_workers, batch_size=args.batch_size)\n",
    "        val_loader = DataLoader(val_ds, num_workers=args.num_workers, batch_size=args.batch_size)\n",
    "\n",
    "        # test dataset\n",
    "        y_test = y[~indices]\n",
    "        x_test = X[~indices]\n",
    "    \n",
    "    # reset classifier models at the beginning of each trial\n",
    "    trunc_multi_log_reg = nn.Linear(in_features=args.IN_FEATURES, out_features=args.K, bias=args.bias)\n",
    "    naive_multi_log_reg = nn.Linear(in_features=args.IN_FEATURES, out_features=args.K, bias=args.bias)\n",
    "    # optimizer and scheduler\n",
    "    trunc_opt = ch.optim.SGD(trunc_multi_log_reg.parameters(), lr=1e-1)\n",
    "    naive_opt = ch.optim.SGD(naive_multi_log_reg.parameters(), lr=1e-1)\n",
    "    # use cosine annealing for learning rate scheduler\n",
    "    trunc_scheduler = ch.optim.lr_scheduler.CosineAnnealingLR(trunc_opt, args.epochs)\n",
    "    naive_scheduler = ch.optim.lr_scheduler.CosineAnnealingLR(naive_opt, args.epochs)\n",
    "    # gradients\n",
    "    trunc_ce_loss = TruncatedGumbelCE.apply\n",
    "    ce_loss = ch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    # train classifier\n",
    "    for epoch in range(args.epochs): \n",
    "        # train loop\n",
    "        trunc_train_loss, trunc_train_acc = Tensor([]), Tensor([])\n",
    "        naive_train_loss, naive_train_acc = Tensor([]), Tensor([])\n",
    "        for batch_X, batch_y in train_loader: \n",
    "            # truncated multinomial regression\n",
    "            trunc_opt.zero_grad()\n",
    "            pred = trunc_multi_log_reg(batch_X)\n",
    "            loss = trunc_ce_loss(pred, batch_y, phi)\n",
    "            loss.backward() \n",
    "            trunc_opt.step()\n",
    "            trunc_scheduler.step()\n",
    "            # keep track of truncated algorithm training loss and accuracy\n",
    "            acc = (ch.argmax(ch.nn.Softmax(dim=1)(pred), dim=1) == batch_y).sum() / batch_y.size(0)\n",
    "            trunc_train_loss = ch.cat([trunc_train_loss, Tensor([loss])]) if trunc_train_loss.size() != ch.Size([0]) else Tensor([loss])\n",
    "            trunc_train_acc = ch.cat([trunc_train_acc, Tensor([acc])]) if trunc_train_acc.size() != ch.Size([0]) else Tensor([acc])\n",
    "            \n",
    "            # naive multinomial regression\n",
    "            naive_opt.zero_grad()\n",
    "            pred = naive_multi_log_reg(batch_X)\n",
    "            loss = ce_loss(pred, batch_y)\n",
    "            loss.backward() \n",
    "            naive_opt.step()\n",
    "            naive_scheduler.step()\n",
    "            # keep track of naive algorithm training loss and accuracy\n",
    "            acc = (ch.argmax(ch.nn.Softmax(dim=1)(pred), dim=1) == batch_y).sum() / batch_y.size(0)\n",
    "            naive_train_loss = ch.cat([naive_train_loss, Tensor([loss])]) if naive_train_loss.size() != ch.Size([0]) else Tensor([loss])\n",
    "            naive_train_acc = ch.cat([naive_train_acc, Tensor([acc])]) if naive_train_acc.size() != ch.Size([0]) else Tensor([acc])\n",
    "        # validation loop\n",
    "        trunc_val_loss, trunc_val_acc = Tensor([]), Tensor([])\n",
    "        naive_val_loss, naive_val_acc = Tensor([]), Tensor([])\n",
    "        with ch.no_grad(): \n",
    "            for batch_X, batch_y in val_loader: \n",
    "                # truncated validation loop\n",
    "                pred = trunc_multi_log_reg(batch_X)\n",
    "                loss = trunc_ce_loss(pred, batch_y, phi)\n",
    "                # keep track of algorithm validation loss and accuracy\n",
    "                acc = (ch.argmax(ch.nn.Softmax(dim=1)(pred), dim=1) == batch_y).sum() / batch_y.size(0)            \n",
    "                trunc_val_loss = ch.cat([trunc_val_loss, Tensor([loss])]) if trunc_val_loss.size() != ch.Size([0]) else Tensor([loss])\n",
    "                trunc_val_acc = ch.cat([trunc_val_acc, Tensor([acc])]) if trunc_val_acc.size() != ch.Size([0]) else Tensor([acc])\n",
    "                \n",
    "                # naive validation loop\n",
    "                pred = naive_multi_log_reg(batch_X)\n",
    "                loss = ce_loss(pred, batch_y)\n",
    "                # keep track of algorithm validation loss and accuracy\n",
    "                acc = (ch.argmax(ch.nn.Softmax(dim=1)(pred), dim=1) == batch_y).sum() / batch_y.size(0)            \n",
    "                naive_val_loss = ch.cat([naive_val_loss, Tensor([loss])]) if naive_val_loss.size() != ch.Size([0]) else Tensor([loss])\n",
    "                naive_val_acc = ch.cat([naive_val_acc, Tensor([acc])]) if naive_val_acc.size() != ch.Size([0]) else Tensor([acc])\n",
    "\n",
    "            # test set accuracy\n",
    "            trunc_test_pred = trunc_multi_log_reg(x_test)\n",
    "            naive_test_pred = naive_multi_log_reg(x_test)\n",
    "            trunc_test_acc = (ch.argmax(ch.nn.Softmax(dim=1)(trunc_test_pred), dim=1) == y_test).sum() / y.size(0)\n",
    "            naive_test_acc = (ch.argmax(ch.nn.Softmax(dim=1)(naive_test_pred), dim=1) == y_test).sum() / y.size(0)\n",
    "                \n",
    "        store[LATENT_CE_TABLE_NAME].append_row({ \n",
    "            'trunc_train_acc': float(trunc_train_acc.mean()), \n",
    "            'trunc_val_acc': float(trunc_val_acc.mean()), \n",
    "            'trunc_train_loss': float(trunc_train_loss.mean()), \n",
    "            'trunc_val_loss': float(trunc_val_loss.mean()),\n",
    "            'naive_train_acc': float(naive_train_acc.mean()), \n",
    "            'naive_val_acc': float(naive_val_acc.mean()), \n",
    "            'naive_train_loss': float(naive_train_loss.mean()), \n",
    "            'naive_val_loss': float(naive_val_loss.mean()),\n",
    "            'trunc_test_acc': float(trunc_test_acc), \n",
    "            'naive_test_acc': float(naive_test_acc),\n",
    "            'epoch': int(epoch + 1),\n",
    "        })\n",
    "    \n",
    "store.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Experiment Data from Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â–Š         | 1/13 [00:00<00:00, 107.98it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The file '/home/pstefanou/MultinomialLogisticRegression/30d572c4-b61d-47c1-9bd1-0c81583b5d8a/store.h5' is already opened, but not in read-only mode (as requested).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-159-75d0fbf4202c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCollectionReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTORE_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLATENT_CE_TABLE_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# close reader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.6/site-packages/cox/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, log_warnings, mode, exp_filter, skip_errs)\u001b[0m\n\u001b[1;32m     51\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error on exp_id {exp_id}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtable_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.6/site-packages/cox/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, log_warnings, mode, exp_filter, skip_errs)\u001b[0m\n\u001b[1;32m     40\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                 \u001b[0mstore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexp_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mthis_table_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.6/site-packages/cox/store.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, storage_folder, exp_id, new, mode)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;31m# Start HDF file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHDFStore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSTORE_BASENAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;31m# Setup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.6/site-packages/pandas/io/pytables.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, mode, complevel, complib, fletcher32, **kwargs)\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fletcher32\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfletcher32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__fspath__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.6/site-packages/pandas/io/pytables.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    678\u001b[0m                 )\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.6/site-packages/pandas/io/pytables.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"can not be written\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.6/site-packages/tables/file.py\u001b[0m in \u001b[0;36mopen_file\u001b[0;34m(filename, mode, title, root_uep, filters, **kwargs)\u001b[0m\n\u001b[1;32m    299\u001b[0m                 raise ValueError(\n\u001b[1;32m    300\u001b[0m                     \u001b[0;34m\"The file '%s' is already opened, but \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m                     \"not in read-only mode (as requested).\" % filename)\n\u001b[0m\u001b[1;32m    302\u001b[0m             \u001b[0;31m# 'a' and 'r+' are compatible with everything except 'r'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0momode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The file '/home/pstefanou/MultinomialLogisticRegression/30d572c4-b61d-47c1-9bd1-0c81583b5d8a/store.h5' is already opened, but not in read-only mode (as requested)."
     ]
    }
   ],
   "source": [
    "reader = CollectionReader(STORE_PATH)\n",
    "results = reader.df(LATENT_CE_TABLE_NAME)\n",
    "reader.close() # close reader\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss and Accuracy Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=results, x='epoch', y='trunc_train_loss', label='trunc train loss')\n",
    "sns.lineplot(data=results, x='epoch', y='naive_train_loss', label='naive train loss')\n",
    "sns.lineplot(data=results, x='epoch', y='trunc_val_loss', color='red', label='trunc val loss')\n",
    "ax = sns.lineplot(data=results, x='epoch', y='naive_val_loss', color='red', label='naive val loss')\n",
    "ax.set(xlabel='epoch', ylabel='CE Loss')\n",
    "plt.show()\n",
    "\n",
    "sns.lineplot(data=results, x='epoch', y='trunc_train_acc', label='trunc train acc')\n",
    "sns.lineplot(data=results, x='epoch', y='naive_train_acc', label='naive train acc')\n",
    "sns.lineplot(data=results, x='epoch', y='trunc_val_acc', label='trunc val acc')\n",
    "ax = sns.lineplot(data=results, x='epoch', y='naive_val_acc', label='naive val acc')\n",
    "ax.set(xlabel='epoch', ylabel='Accuracy')\n",
    "plt.show()\n",
    "\n",
    "sns.lineplot(data=results, x='epoch', y='trunc_test_acc', label='trunc test acc')\n",
    "ax = sns.lineplot(data=results, x='epoch', y='naive_test_acc', label='naive test acc')\n",
    "ax.set(xlabel='epoch', ylabel='Test Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
